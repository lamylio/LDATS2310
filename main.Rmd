---
title: "Main"
author: "Lamy Lionel"
date: "10/11/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r message=F, warning=F}
library(MASS) # negative.binomial
library(boot) # GLM CV
library(mgcv) # GAM
library(pscl) # zeroinfl
library(gbm)  # GBM

library(ggplot2)
library(dplyr)
library(tidyr)
#library(countreg)

library(rpart) # Tree and RF
library(rpart.plot)

library(glmnet)
library(caret)
library(jtools)

```

### Conversion into factors 

```{r}
base_test <- read.table("./data/DBtest.csv", sep=",", header=TRUE)
base_train = read.table("./data/DBtrain.csv", sep=",", header=TRUE)

base_train = within(base_train, {
  X = NULL
  Gender = factor(Gender, labels=c("M", "F"))
  Area = factor(Area, labels=c("Suburban", "Urban", "Countryside low", "Coutryside high"))
  Leasing = factor(Leasing, labels=c("Yes", "No"))
  Power = factor(Power, labels=c("Low", "Normal", "Intermediate", "High"))
  Contract = factor(Contract,  labels=c("Basic", "Intermediate", "Full"))
  Fract = factor(Fract, labels=c("Monthly", "Quarterly", "Yearly"))
  #Hasclaim = factor(base_train$Nbclaims > 0, labels=c("No", "Yes"))
  
})

base_train_ordered = within(base_train, {
  Power = factor(Power, labels=c("Low", "Normal", "Intermediate", "High"), ordered = T)
  Contract = factor(Contract,  labels=c("Basic", "Intermediate", "Full"), ordered = T)
})

base_test = within(base_test, {
  X = NULL
  Gender = factor(Gender, labels=c("M", "F"))
  Area = factor(Area, labels=c("Suburban", "Urban", "Countryside low", "Coutryside high"))
  Leasing = factor(Leasing, labels=c("Yes", "No"))
  Power = factor(Power, labels=c("Low", "Normal", "Intermediate", "High"))
  Contract = factor(Contract, labels=c("Basic", "Intermediate", "Full"))
  Fract = factor(Fract, labels=c("Monthly", "Quarterly", "Yearly"))
  
})

base_test_ordered = within(base_test, {
  Power = factor(Power, labels=c("Low", "Normal", "Intermediate", "High"), ordered = T)
  Contract = factor(Contract,  labels=c("Basic", "Intermediate", "Full"), ordered = T)
})

base_train_freq = base_train
base_train_freq$Frequency = base_train$Nbclaims / base_train$Exposure

base_train_nonull = base_train[base_train$Nbclaims > 0,]

```


```{r}
# Custom breaks for GLM. We'll use GAM later.
glm_train = base_train
glm_test = base_test

glm_break_driver = c(18,25,35,50,65,80,100) # 6
glm_break_car = c(0,3,6,9,12,15,20) # 6

glm_train$DriverAge <- cut(glm_train$DriverAge, breaks = glm_break_driver, right=F)
glm_test$DriverAge <- cut(glm_test$DriverAge, breaks = glm_break_driver, right=F)
glm_train$CarAge <- cut(glm_train$CarAge, breaks = glm_break_car, right = F)
glm_test$CarAge <- cut(glm_test$CarAge, breaks = glm_break_car, right = F)
```

```{r}
# ---
formule.covariates = c("Gender", "DriverAge", "CarAge", "Area", "Leasing", "Power", "Fract", "Contract")

formule.long.offset = as.formula(paste("Nbclaims ~ offset(log(Exposure)) + ", paste(formule.covariates, collapse=" + ")))

formule.long = as.formula(paste("Nbclaims ~ ", paste(formule.covariates, collapse=" + ")))

formule.step = as.formula(Nbclaims ~ Gender + DriverAge + Area + Leasing + 
    Power + Fract + offset(log(Exposure)))

customDeviance = function(fitt){
  y = base_train_freq$Frequency
  w = base_train$Exposure
  
  left = y * log(y) - y * log(fitt)
  right = y - fitt
  
  left[y==0] = 0 # Replace NA's by 0 because log(0) is -Inf but 0*Inf ~ 0
  
  return(2*sum(w*(left-right)))
}

```

```{r}
models.comparison = data.frame()

addComparison = function(model, model_name){
  to_combine = data.frame(
      LogLik = logLik(model)[1],
      AIC = AIC(model),
      BIC = BIC(model),
      #Deviance = deviance(model)|0,
      Deviance.custom = customDeviance(model$fitted),
      #Percent.of.zero = round(mean(exp(-exp(predict(model))))*100,2),
      #Range.Predict = range(exp(predict(model))),
      Count.0 = round(sum(dpois(0, model$fitted))),
      Count.1 = round(sum(dpois(1, model$fitted))),
      Count.2 = round(sum(dpois(2, model$fitted))),
      Count.3 = round(sum(dpois(3, model$fitted))),
      Count.4 = round(sum(dpois(4, model$fitted))),
      row.names = as.character(model_name)
  )
  rbind(
    models.comparison,
    to_combine
  )
}
```

### GLM Prediction

```{r}
model.glm.1 = glm(formule.long.offset, data = base_train, family=quasipoisson(link = "log"))
model.glm.2 = glm(formule.long.offset, data = base_train, family="poisson")

model.glm.3 = glm(formule.short.offset, data = base_train, family="poisson")
model.glm.4 = glm(formule.step, data = glm_train, family="poisson")

model.glm.3 = glm(formule.long, data = glm_train, family="poisson", offset = log(Exposure))

model.nb.1 = glm.nb(formule.long.offset, data = base_train, control = glm.control(trace = 2, maxit = 100))
model.nb.2= glm.nb(formule.long, data = base_train, control = glm.control(trace = 2, maxit = 100))

model.zero.1 = zeroinfl(formule.long.offset, data=base_train, dist="negbin")
model.zero.2 = zeroinfl(formule.long, data=glm_train, dist="negbin", offset = log(Exposure))

model.hurdle.1 = hurdle(formule.long.offset, data=base_train, dist="negbin", zero.dist = "poisson")
model.hurdle.2 = hurdle(formule.long.offset, data=base_train, dist="negbin", zero.dist = "binomial")

# Hurdle with negbin is better!!

```

```{r}
# Bagging GLM

model.glm.bagging = (function(B = 50){
  
  nr = nrow(base_train)
  nrTest = nrow(base_test)
  
  allPredicted = matrix(NA, B, nrTest)
  
  for (i in 1:B){
    
    samplePick = sample(nr, nr, T)
    sampleTrain = base_train[samplePick,]
    
    sampleModel = glm(formule.long.offset, data = base_train, family="poisson")
    
    allPredicted[i, ] = predict(sampleModel, base_test, type="response")
    
  }
  
  return (colMeans(allPredicted))
  
})()
```


#### Comparison of the glm models.

```{r}
models.comparison = addComparison(model.glm.1, "GLM 1")
models.comparison = addComparison(model.glm.2, "GLM 2")
models.comparison = addComparison(model.glm.3, "GLM 3")
models.comparison = addComparison(model.glm.4, "GLM 4")
models.comparison = addComparison(model.zero.1, "GLM Z 1")
models.comparison = addComparison(model.hurdle.1, "GLM H 1")
models.comparison = addComparison(model.hurdle.2, "GLM HNB 1")
models.comparison = addComparison(model.nb.1, "GLM NB 1")
models.comparison = addComparison(model.nb.2, "GLM NB 2")
```

#### Cross validation (by deviance)

```{r}
# https://stackoverflow.com/questions/44706961/output-of-cv-glm-vs-cv-glmnet
cost_deviance <- function(y, eta) {
  deveta = y * log(eta) - eta
  devy = y * log(y) - y
  devy[y == 0] = 0
  sum(2 * (devy - deveta))
}


model.glm.1.cv = cv.glm(base_train, model.glm.1, cost_deviance, K=10)$delta
model.glm.2.cv = cv.glm(glm_train, model.glm.2, cost_deviance, K=10)$delta
model.glm.3.cv = cv.glm(glm_train, model.glm.3, cost_deviance, K=10)$delta
model.glm.4.cv = cv.glm(base_train, model.glm.4, cost_deviance, K=10)$delta

models.glm.cv = data.frame(model.glm.1.cv, model.glm.2.cv, model.glm.3.cv, model.glm.4.cv)
```

### --------------
### GAM

```{r}

model.gam.1 = gam(
    Nbclaims ~ s(DriverAge, bs="cr", by = Gender) + Power+Gender+Area+Leasing+Fract,
    data = base_train, 
    family=poisson(),
    offset = log(Exposure),
    method = "REML"
)

model.gam.2 = gam(Nbclaims ~ s(DriverAge, by=Gender) + s(CarAge) +Power+Gender+Area+Leasing+Fract,
    data = base_train, 
    family=poisson(),
    offset=log(Exposure),
    method = "REML"
)

model.gam.3 = gam(
    Nbclaims ~ s(DriverAge, bs="cr", k=6) + Power+Gender+Area+Leasing+Fract,
    data = base_train, 
    family=poisson(),
    offset = log(Exposure),
    method = "REML"
)

model.gam.4 = gam(
    Nbclaims ~ s(DriverAge, bs="cr") + Power+Gender+Area+Leasing+Fract + s(CarAge, bs="cr"),
    data = base_train, 
    family=poisson(),
    offset = log(Exposure),
    method = "REML"
)

model.gam.1.predicted = predict(model.gam.1, type="response")
model.gam.2.predicted = predict(model.gam.2, type="response")
model.gam.3.predicted = predict(model.gam.3, type="response")
model.gam.4.predicted = predict(model.gam.4, type="response")


plot(model.gam.3, rug = T, se = T, shade = T, shade.col = 'gray90')

models.comparison = addComparison(model.gam.1, "GAM 1")
models.comparison = addComparison(model.gam.2, "GAM 2")
models.comparison = addComparison(model.gam.3, "GAM 3")
models.comparison = addComparison(model.gam.4, "GAM 3")
```

#### Regression tree (RT)

```{r}
# Factors are ordered
model.tree.1 = rpart(
  cbind(Exposure, Nbclaims) ~ Gender + DriverAge + CarAge + Power + Area + Fract + Leasing, 
  data=base_train_ordered,
  method = "poisson",
  parms = list(shrink=4), # Method is deviance
  control = rpart.control(cp = 0.001, xval = 10, maxdepth = 20)
)

rpart.plot(model.tree.1, digits=2, type=4)

model.tree.1.predicted = predict(model.tree.1)

# Bagging
model.tree.2.predicted = (function(M = 20){
  nr = nrow(base_train) # size of the dataset
  nrTest = nrow(base_test)
  
  allPredict = matrix(0, M, nrTest)
  
  for (i in 1:M){
    
    samplePick = sample(nr, nr, T)
    sampleTrain = base_train[samplePick,]
    
    sampleTree = rpart(
      cbind(Exposure, Nbclaims) ~ Gender + DriverAge + CarAge + Power, 
      data=sampleTrain,
      method = "poisson",
      control = rpart.control(cp = 0.001, xval = 10)
    )
    allPredict[i,] = predict(sampleTree, base_test)
  }
  
  return (colMeans(allPredict))
})()
```

# Random Forest

```{r}


model.forest.1 = (function(M=100, C=3){
  # C is number of covariates picked
  nr = nrow(base_train)
  nrTest = nrow(base_test)
  sampleSize = nr
  
  
  allResults = matrix(NA, M, nrTest)
  for (i in 1:M){
    
    trainSample = sample(nr, sampleSize, replace=T)
    trainSample = base_train[trainSample,]
    
    covSample = sample(formule.covariates, C)
    equation = paste("cbind(Exposure, Nbclaims) ~ ", paste(covSample, collapse = " + "))
    
    tree = rpart(equation, data=trainSample, method="poisson", control=rpart.control(cp=0.0001, xval = 10))
    allResults[i,] = predict(tree, base_test)
  }
  
  return (colMeans(allResults))
})()

# This one is parametric (poisson)
model.forest.2 = (function(M=100, C=3){
  # C is number of covariates picked
  
  baseTree = model.tree.1 # See above
  
  nr = nrow(base_train)
  nrTest = nrow(base_test)
  sampleSize = nr
  
  
  allResults = matrix(NA, M, nrTest)
  for (i in 1:M){
    
    baseFit = predict(baseTree, base_train)
    baseFitNbclaims = rpois(nr, baseFit*base_train$Exposure)
    
    trainSample = base_train
    trainSample$Nbclaims = baseFitNbclaims # Replace by predicted
    
    covSample = sample(formule.covariates, C)
    equation = paste("cbind(Exposure, Nbclaims) ~ ", paste(covSample, collapse = " + "))
    
    tree = rpart(equation, data=trainSample, method="poisson", control=rpart.control(cp=0.001, xval = 10))
    allResults[i,] = predict(tree, base_test_ordered)
  }
  
  return (colMeans(allResults))
})()
```

# Gradient boosting model

```{r}
model.gbm.1 = gbm(formule.long.offset, data=base_train_ordered, distribution = "poisson", shrinkage = 0.001, n.trees = 2000, n.cores = 4, cv.folds = 3, train.fraction = 0.7)

model.gbm.2 = gbm(formule.long.offset, data=base_train_ordered, distribution = "poisson", shrinkage = 0.001, n.trees = 2000, n.cores = 4, cv.folds = 3, interaction.depth = 2)
```

# MOB Random forest

```{r}
library(mobForest)


model.mob = mobforest.analysis(formule.long.offset, data=base_train, family = poisson(), partition_vars = formule.covariates)

```

```{r}
rfPoisson_train = within(base_train, {
    
  Nbclaims = NULL
  Exposure = NULL
  Frequency = NULL
  
})

model.rfCount = rfPoisson(x = rfPoisson_train, y=base_train$Nbclaims, offset = log(base_train$Exposure),
                       ntree=100, nodesize=1, maxnodes=30, do.trace=T, replace = T)
```


